#!/usr/bin/env node

// It expects you have these softwares available on your machine:
// - gdal

const {promises:fs} = require('fs')
const {createWriteStream} = require('fs')
const {promisify} = require('util')
const pipeline = promisify(require('stream').pipeline)
const slugify = require('@sindresorhus/slugify')
const ora = require('ora')
const Listr = require('listr')
const {get} = require('axios')
const gdal = require('gdal-next')
const {convert} = require('geojson2shp')
const {basename, dirname, join, resolve} = require('path')
const { featureCollection } = require('@turf/helpers')
const glob = require('globby')
const argv = require('minimist')(process.argv.slice(2), {
  string: ['epci', 'departement', 'insee', 'millesime', 'name'],
  boolean: ['in-lambert-93', 'shapefiles'],
  alias: {
    inLambert93: 'in-lambert-93'
  },
  default: {
    shapefiles: true,
    inLambert93: false,
    name: '',
    insee: '',
    departement: ''
  }
})

const workerFarm = require('worker-farm')
const {Worker} = require('worker_threads')
const shapefileWorker = require.resolve('../src/workers/shapefile.js')
const extract = promisify(workerFarm({maxConcurrentCallsPerWorker: 1, maxRetries: 0}, shapefileWorker))

const {_:FILTERING_FILES, epci, departement: departements, insee, millesime} = argv
const {shapefiles, inLambert93} = argv
const {from:SOURCE_FILES} = argv
const {name} = argv

const WGS_84 = 4326

let DESTINATION_FILE = null


if (!FILTERING_FILES || !SOURCE_FILES || !millesime) {
  console.error('npm run export -- --millesime 2019 --from "../RPG/2019/d011/*.shp" --shapefiles path/to/*.shp')
  console.error('npm run export -- --millesime 2019 --from "../RPG/2019/d*/*.shp" --shapefiles path/to/*.shp')
  console.error('npm run export -- --millesime 2019 --from "../RPG/2019/d*/*.shp" --shapefiles --in-lambert-93 path/to/*.shp')
  console.error('npm run export -- --millesime 2019 --from "../RPG/2019/d064/*.shp" --epci 200067106')
  console.error('npm run export -- --millesime 2019 --from "../RPG/2019/d026/*.shp" --insee 26108,26011,26289')
  console.error('npm run export -- --millesime 2019 --from "../RPG/2019/{d026,d007}/*.shp" --departement 26,07')
  process.exit(1)
}

const uploadDir = (file) => join(__dirname, '..', 'exports', file)

const splitCliArg = (arg) => String(arg).split(',').map(code => code.trim())

function pushFeaturesIntoDataset({ features, crs, datasets }) {
  const dataset = gdal.open(uploadDir('input.gpkg'), 'w', 'GPKG')

  const layer = dataset.layers.create('communes', crs, gdal.wkbPolygon)
  features.forEach(({ geometry, properties }) => {
    const feature = new gdal.Feature(layer)

    feature.setGeometry(gdal.Geometry.fromGeoJson(geometry))
    feature.fields.set(properties)

    layer.features.add(feature)
  })

  datasets.push(dataset)
}

function fetchJSON(url) {
  const FILENAME = basename(url)
  const DEST = join(__dirname, '..', 'public', FILENAME)

  // we pipe the source file to the local filesystem (createWriteStream())
  // if it does not exist (catch())
  return fs.stat(DEST)
    .catch(async () => {
      const response = await get(url, {responseType: 'stream'})

      return pipeline(response.data, createWriteStream(DEST))
    })
    .catch(error => {
      fs.unlink(DEST)
      throw error
    })
    .then(() => fs.readFile(DEST, {encoding: 'utf8'}))
    .then((geojson) => JSON.parse(geojson))
}

/**
 * Return features linked by their Code INSEE
 *
 * @param  {[type]} collection [description]
 * @param  {[type]} codes      [description]
 * @return {[type]}            [description]
 */
async function getFeaturesByProperty ({ collection: featureCollectionP, comparisonFn }) {
  const collection = await featureCollectionP

  return collection.features
    .filter(({ properties }) => comparisonFn(properties))
}

async function run (FILTERING_FILES) {
  const l93 = gdal.SpatialReference.fromProj4('+init=epsg:2154')
  const wgs84 = gdal.SpatialReference.fromProj4('+init=epsg:4326')
  const l93towgs84 = new gdal.CoordinateTransformation(l93, wgs84)
  let datasets = [];

  const filteringFiles = await glob(FILTERING_FILES)
  const sourceFiles = await glob(SOURCE_FILES)
  const datasetCount = filteringFiles.length + (epci ? 1 : 0) + (insee.split(',').length)

  // X. Create export directory if needed
  await fs.mkdir(uploadDir(''), { recursive: true })

  // 1. Open datasets
  // shapefiles
  let spinner = ora({
    text: `Parsing ${datasetCount} shapefiles`,
    spinner: 'simpleDotsScrolling'
  }).start()

  // 1.a Filter by EPCI or Insee Code boundaries
  if (epci || insee) {
    let codes = []

    if (epci) {
      const { nom, codes:inseeCodes } = await fetchJSON('https://unpkg.com/@etalab/decoupage-administratif@0.8.0/data/epci.json')
        .then(response => response.data)
        .then(list => list.find(({ code }) => code === String(epci)))
        .then(({ nom, membres }) => ({ nom, codes: membres.map(m => m.code) }))
      codes.push(...inseeCodes)

      DESTINATION_FILE = DESTINATION_FILE ?? `export-${slugify(nom) || `epci-${epci}`}-${millesime}`
    }
    else if (insee) {
      const inseeCodes = splitCliArg(insee)
      codes.push(...inseeCodes)
      DESTINATION_FILE = DESTINATION_FILE ?? `export-${slugify(name) || `insee-${inseeCodes.join(',')}`}-${millesime}`
    }

    const features = await getFeaturesByProperty({
      collection: fetchJSON('https://github.com/gregoiredavid/france-geojson/raw/v2.1.1/communes.geojson'),
      comparisonFn: (properties) => codes.includes(properties.code)
    })

    pushFeaturesIntoDataset({ features, datasets, crs: wgs84 })
  }

  // 1.b Or, filter by departement
  if (departements) {
    const codes = splitCliArg(departements).map(d => d.padStart(2, '0'))

    const features = await getFeaturesByProperty({
      collection: fetchJSON('https://github.com/gregoiredavid/france-geojson/raw/v2.1.1/departements-avec-outre-mer.geojson'),
      comparisonFn: (properties) => codes.includes(properties.code)
    })

    pushFeaturesIntoDataset({ features, datasets, crs: wgs84 })
    DESTINATION_FILE = DESTINATION_FILE ?? `export-${slugify(name) || codes.map(code => `d${code}`).join(',')}-${millesime}`
  }

  // 1.c Or, filter by shapefile
  if (shapefiles && filteringFiles.length) {
    const firstFile = basename(filteringFiles[0]).toLocaleLowerCase()

    filteringFiles
      .map(path => resolve(path))
      .map(path => gdal.open(path, 'r'))
      .forEach(dataset => datasets.push(dataset))

    DESTINATION_FILE = DESTINATION_FILE ?? `export-${slugify(name || firstFile)}-${millesime}`
  }

  // 2. Collect all the features from each layer
  const filteringFeatures = datasets.reduce((features, ds) => {
    ds.layers.forEach(layer => {
      layer.features.forEach((feature) => {
        const geometry = feature.getGeometry()
        inLambert93 && geometry.transform(l93towgs84)
        features.push(geometry.toObject())
      })
    })

    return features
  }, [])

  spinner.succeed(`Parsed ${filteringFeatures.length} features in ${datasetCount} datasets.`)

  // 3. Browse bio/non-bio layers and intersect features
  const allFeatures = sourceFiles
    .map(s => resolve(s))
    .map(sourceFile => ({
      sourceFile,
      featuresP: extract({sourceFile, filteringFeatures, millesime})
    }))

    // const worker = new Worker(
    //   shapefileWorker,
    //   {workerData: {sourceFile, filteringFeatures}}
    // )
    //
    // worker.on('message', () => featuresCount++)
    // worker.on('error', reject)

  const tasks = new Listr([
    {
      title: 'Filtering features',
      task: () => new Listr(
        allFeatures.map(({sourceFile, featuresP}) => ({
          title: `${basename(dirname(sourceFile))}/${basename(sourceFile)}`,
          task: () => featuresP.then(features => {
            return `${features.length.toFixed(0).padStart(5)} features`
          })
        })),
        { concurrent: true }
      )
    }
  ])

  tasks.add({
    title: `Exporting as ${DESTINATION_FILE}.geojson`,
    task: (ctx) => {
      // 4.1 Convert/Intersect features and aggregate them into the `features` collection
      return Promise
          .all(allFeatures.map(({featuresP}) => featuresP))
          .then(allFeatures => [].concat(...allFeatures))
          .then(features => {
            ctx.features = features
            return fs.writeFile(uploadDir(`${DESTINATION_FILE}.geojson`), JSON.stringify(
              featureCollection(features),
              null,
              2))
            })
    }
  })

  // XX. â€¦ and close datasets
  tasks.add({
    title: 'Cleanup',
    task: () => {
      datasets.forEach(d => d.close())
      // return fs.unlink(uploadDir('input.gpkg'))
    }
  })

  // 6. Converts as an ESRI Shapefile
  tasks.add({
    title: `Exporting as ${DESTINATION_FILE}.shp.zip`,
    task: (ctx) => {
      return convert(featureCollection(ctx.features), uploadDir(`${DESTINATION_FILE}.shp.zip`), {
        layer: DESTINATION_FILE,
        targetCrs: WGS_84
      })
    }
  })

  await tasks.run()
  process.exit(0)
}

run(FILTERING_FILES)
